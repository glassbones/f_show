Everyone knows the headline:
AI demand ‚Üí hyperscalers buy every GPU NVIDIA can make.

But here are the less‚Äëadvertised facts that matter more now:

1. GPUs aren‚Äôt the bottleneck anymore.

For the first time, GPU production is catching up to orders‚Äî
but the infrastructure around GPUs is falling behind.

This means:
Even if you have thousands of Blackwells or Rubins,
they can‚Äôt run at full speed because the rest of the system can‚Äôt keep up.

<img width="1024" height="559" alt="image_48c6143b-ffa0-41b5-9c22-2f6515a05f55" src="https://github.com/user-attachments/assets/4ea84053-2bab-4887-b0fd-93e4965c9ef1" />


2. The real choke points are now three things:

A. HBM (High Bandwidth Memory)

This is the ultra‚Äëfast memory stacked on the GPU.
Without enough HBM bandwidth, GPUs literally sit idle.

B. Optical Interconnects (800G ‚Üí 1.6T transceivers)

GPUs need to talk to each other ‚Äî a lot.
NVIDIA‚Äôs newest clusters require laser‚Äëbased fiber optics,
not copper cables.
There aren‚Äôt enough optical modules to go around.

C. Advanced Packaging

This is how you physically bond GPUs to HBM.
It‚Äôs insanely complex.
There‚Äôs not enough domestic capacity, and Asia holds the lead.

These three constraints are the ‚Äúplumbing‚Äù of AI.
You don‚Äôt see them in flashy keynotes, but they dictate what can actually be built.


---

Why these bottlenecks exist (in plain language)

GPUs evolved faster than the infrastructure that feeds them.

In 2023‚Äì2024 the world panic‚Äëbought GPUs.

But now in 2025‚Äì2027 the industry has hit physics‚Äëlevel limits:

GPUs can compute faster than HBM can feed them

GPU clusters need optical bandwidth far beyond legacy data center networks

Packaging cannot scale with the complexity of HBM4 and Rubin‚Äëclass GPUs


We‚Äôve essentially built ‚Äúsuper‚Äëengines‚Äù without building the rest of the car.


---

O ‚Äî ORIENT (What These Observations Mean)

Here‚Äôs the critical insight:

> The value in AI is shifting from compute ‚Üí to the infrastructure that enables compute.



Everyone already owns NVIDIA.
Everyone already knows the GPU story.

But the second order story is only beginning:

1. HBM Becomes the New Oil

Supply extremely tight.
Demand vertical.
Only three real global suppliers.
And the U.S. wants domestic control ‚Üí Micron gets uplift.

2. Optics Become the Nervous System of AI

AI clusters now need so much bandwidth that
every single GPU requires multiple high‚Äëspeed transceivers.
A single Rubin cluster needs tens of thousands of 800G/1.6T lasers.

This is not optional.
It‚Äôs mandatory.

3. Packaging Is a Silent Crisis

You cannot ship AI chips without advanced packaging.
It‚Äôs an invisible bottleneck that dictates how many Rubins or Blackwells you can actually build.

This is where Amkor, ASE, and material suppliers gain leverage.


---

How to explain this simply to anyone familiar with GPUs:

> Phase 1: Buy GPUs ‚Üí stock explodes (2023‚Äì2024)
Phase 2: You plug them in ‚Üí networks choke, memory bottlenecks, thermal limits (2025‚Äì2027)
Phase 3: Market realizes the real constraint isn‚Äôt compute ‚Äî it‚Äôs bandwidth + packaging
Phase 4: Capital rotates into the companies that solve the bottlenecks

Here is PART TWO ‚Äî DECIDE + ACT
This is the critical‚Äëpath OODA continuation, written with the same clean, high‚Äëdensity, no‚Äëbullshit style as Part One.


---

OODA ‚Äî PART II (DECIDE + ACT)

‚ÄúGiven the bottlenecks, what does a rational strategist do next?‚Äù


---

D ‚Äî DECIDE

The Critical Path Insight (the thing most investors haven‚Äôt internalized yet):

> The next wave of AI value will accrue to the companies that remove the bottlenecks, not the companies that make the GPUs.



Everyone understands GPUs.
But the constraints that determine how many GPUs can actually run at full speed are:

1. HBM bandwidth


2. Optical interconnect bandwidth


3. Advanced packaging capacity


4. Domestic/secure supply chain pressure (U.S. onshoring)



These four factors decide:

who can build AI clusters

how fast they run

and which suppliers become kingmakers in 2026‚Äì2028


This is where the money is moving next.

The compute boom is Phase 1.
The bandwidth + packaging boom is Phase 2.
We‚Äôre sitting at the doorstep of Phase 2 right now.


---

A ‚Äî ACT (THE BETS)

These are the specific, actionable bets aligned with where the ball is actually moving.

Below is a structured list:
Primary Bets ‚Üí Secondary Bets ‚Üí Stability Layer


---

üî• PRIMARY BETS (Highest Conviction)

These are the companies that sit directly in the flow of the bottlenecks.

1. Coherent (COHR) ‚Äî optics / lasers

Thesis: AI clusters NEED 1.6T optical transceivers. Lasers = mandatory.
Why it hits: U.S. wants domestic photonics ‚Üí COHR is the tip of the spear.

2. Lumentum (LITE) ‚Äî photonics / CPO

Thesis: U.S.-based, already expanding advanced optics manufacturing.
Why it hits: Cloud + DoD + DOE want trusted, non‚ÄëChinese optics.

3. Fabrinet (FN) ‚Äî optical module manufacturing

Thesis: If optics volume explodes, Fabrinet prints money.
Why it hits: They build the modules for almost all major vendors.

4. Marvell (MRVL) ‚Äî DSPs inside 800G/1.6T optics

Thesis: DSPs become the ‚Äútax‚Äù on bandwidth.
Why it hits: Every transceiver needs Marvell silicon.

5. Micron (MU) ‚Äî U.S. HBM onshoring

Thesis: CHIPS Act + U.S. national security = Micron is the chosen domestic memory vendor.
Why it hits: Governments and hyperscalers MUST diversify away from Asia for HBM.


---

üì¶ SECONDARY BETS (Medium Conviction, Enablers of Phase 2)

6. Amkor (AMKR) ‚Äî advanced packaging

Thesis: HBM4 + Rubin packaging bottlenecks ‚Üí onshore packaging becomes gold.
Why it hits: Amkor already building U.S. capacity.

7. Entegris (ENTG) ‚Äî packaging & HBM materials

Thesis: Every advanced HBM line requires ENTG materials to hit yield targets.
Why it hits: Materials = picks & shovels at scale.

8. Lam Research (LRCX) / KLA (KLAC) ‚Äî HBM yield tools

Thesis: Complexity of HBM4 ‚Üí huge demand for etch, metrology.
Why it hits: These toolmakers get paid in every memory expansion.


---

üõ°Ô∏è STABILITY LAYER (Lower Risk, Still Directly in the Flow)

9. Broadcom (AVGO) ‚Äî networking silicon

Thesis: Rubin clusters need next‚Äëgen switching + SerDes bandwidth.
Why it hits: AVGO is the least avoidable part of AI networking.

10. Applied Materials (AMAT) ‚Äî packaging tools

Thesis: Advanced packaging grows faster than wafers.
Why it hits: Safe exposure to the packaging arms race.


---

SIMPLE SUMMARY:

‚ÄúGPUs were phase one.
The next phase is fixing the memory, bandwidth, and packaging bottlenecks that are choking AI clusters.
These companies are the ones who solve that problem.‚Äù

These bets are not about hype.
They‚Äôre about physics, manufacturing reality, and U.S. industrial policy.
